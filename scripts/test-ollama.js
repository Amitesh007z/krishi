#!/usr/bin/env node

const fetch = require('node-fetch');

async function testOllama() {
  console.log('ðŸ” Testing Ollama connection...\n');
  
  try {
    // Test if Ollama is running
    const response = await fetch('http://localhost:11434/api/tags', {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' },
      timeout: 5000
    });
    
    if (response.ok) {
      const data = await response.json();
      const models = data.models || [];
      
      console.log('âœ… Ollama is running successfully!');
      console.log(`ðŸ“¦ Available models: ${models.length}`);
      
      if (models.length > 0) {
        console.log('ðŸ“‹ Models:');
        models.forEach(model => {
          console.log(`   - ${model.name} (${model.size || 'Unknown size'})`);
        });
        
        // Test if llama2:7b is available
        const hasLlama2 = models.some(m => m.name.includes('llama2:7b'));
        if (hasLlama2) {
          console.log('\nðŸŽ‰ Perfect! llama2:7b is available and ready to use.');
          return true;
        } else {
          console.log('\nâš ï¸  Warning: llama2:7b not found. Please run: ollama pull llama2:7b');
          return false;
        }
      } else {
        console.log('\nâš ï¸  No models found. Please run: ollama pull llama2:7b');
        return false;
      }
    } else {
      console.log('âŒ Ollama responded but with error:', response.status);
      return false;
    }
    
  } catch (error) {
    console.log('âŒ Ollama is not running or not accessible');
    console.log('ðŸ’¡ To fix this:');
    console.log('   1. Install Ollama: https://ollama.ai/download');
    console.log('   2. Start Ollama: ollama serve');
    console.log('   3. Download model: ollama pull llama2:7b');
    return false;
  }
}

// Run the test
testOllama().then(success => {
  process.exit(success ? 0 : 1);
});
